---
title: "Class 08"
author: "Amy Prichard"
date: "February 6, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Clustering

K-means first play

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)

```

```{r}
km <- kmeans(x, 2, nstart=20)
km
```

Size of clusters
```{r}
km$size
```

Cluster membership vector
```{r}
km$cluster
```

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=3)  # cluster centers (blue squares)
      # cex ("character expansion") changes the size of the point on the plot
```

## Hierarchical clustering in R 

```{r}
# First we need to calculate point (dis)similarity
#   as the Euclidean distance between observations
dist_matrix <- dist(x) 

# The hclust() function returns a hierarchical 
#   clustering model
hc <- hclust(d = dist_matrix)

# the print method is not so useful here
hc  
```

```{r}
plot(hc)
abline(h=6, col="red")   # marks dendrogram at height=6
grp2 <- cutree(hc, h=6)  # cuts dendrogram at height=6
```

```{r}
plot(x, col=grp2)  # shows where the two groups from the dendrogram are
```

```{r}
# another example
plot(hc)
abline(h=2.5, col="blue")  # looks like 14 groups
grp14 <- cutree(hc, h=2.5)
table(grp14)
plot(x, col=grp14)
```

```{r}
# ...but I want 6 groups!
grp6 <- cutree(hc, k=6)  # cut into k groups
plot(x, col=grp6)
```

## Linkage clusters in hierarchical clustering

COMPLETE: largest possible distance
SINGLE: smallest possible distance
AVERAGE: mean of all distances
CENTROID: distance between centroids

```{r}
# Using different hierarchical clustering methods
hc.complete <- hclust(dist_matrix, method="complete")
plot(hc.complete)
hc.average  <- hclust(dist_matrix, method="average")
plot(hc.average)
hc.single   <- hclust(dist_matrix, method="single")
plot(hc.single)
hc.centroid <- hclust(dist_matrix, method="centroid")
plot(hc.centroid)
```


```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters 
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) ) 
plot(x, col=col)
```

# Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters.
```{r}
hc <- hclust(dist(x))
plot(hc)
abline(h=2.5, col="green")  # 2
x_2 <- cutree(hc, h=2.5)    # 2
abline(h=2, col="magenta")  # 3
x_3 <- cutree(hc, h=2)      # 3

plot(x, col=x_2, main="2")  # 2
plot(x, col=x_3, main="3")  # 3
```

# How does this compare to your known 'col' groups?
Known 'col' groups is more similar to 3 than to 2. But some changes. Outliers have been grouped into groups that better fit their outlier-ness than their original group (i.e. there are no outliers because they've been grouped away).

```{r}
# Example code that does the same thing
hc <- hclust(dist(x))
plot(hc)

grps <- cutree(hc, k=3)  # oh yeah, k is an argument that exists...
plot(x, col=grps)
# This is better anyway (esp. for random data) because it's more precise
```



## Principal Component Analysis

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV", row.names=1) 
head(mydata) 
```

```{r}
## let's do PCA!
pca <- prcomp(t(mydata), scale=TRUE)
             # transpose mydata so that rows and columns are set up for prcomp()
summary(pca)
```

Make PCA plot

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", main="PCA")
    # PC1       # PC2
# PC1, 2, 3, etc are always numbered in order of importance
```

```{r}
## Percent variance is often more informative to look at
pca.var <- pca$sdev^2
round(pca.var/sum(pca.var)*100, 1)  # round to one decimal place
pca.var.per <- round(pca.var/sum(pca.var)*100, 1) 
```

```{r}
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```

Make our PCA plot nice

```{r}
## A vector of colors for wt and ko samples 
colvec <- as.factor(substr(colnames(mydata),1,2))

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
	xlab=paste0("PC1 (", pca.var.per[1], "%)"),
	ylab=paste0("PC2 (", pca.var.per[2], "%)"))
```



### PCA of UK food data

```{r}
x <- read.csv("data/UK_foods.csv")
```

# Q1: How many rows and columns are in your new data frame named x? What R functions could you use to answer this question?

```{r}
nrow(x)
ncol(x)
```

```{r}
# alternatively...
dim(x)
```


## Checking your data

```{r}
head(x)
```

```{r}
# The row names are incorrectly set as the first column of x
# Fix this by setting row names to the first column, then remove the troublesome first column

rownames(x) <- x[,1]
x <- x[,-1]  # note how the minus indexing works - DON'T RUN THIS TWICE!
head(x)

# In the future, fix this by setting row.names=1 in the read.csv() function...
```

```{r}
dim(x)
```

# Q2: Which approach to solving the 'row-names problem' mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

```{r}
# This is more concise, and it also avoids complications with losing columns!
x <- read.csv("data/UK_foods.csv", row.names=1)
head(x)
```

## Spotting major differences and trends

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

# Q3: Changing what optional argument in the above barplot() function results in a plot with stacked bars?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))  # beside=FALSE
```

# There is no Q4...

# Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
# Word boxes show axis titles
# On the diagonal = no difference between two groups
```

# Q6: What is the main differences between Northern Ireland and the other countries of the UK in terms of this data-set?

I don't know... let's try PCA!

## PCA to the rescue

```{r}
# Use the prcomp() PCA function 
pca <- prcomp(t(x))
summary(pca)
```

# Q7: Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

# Q8: Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at the start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))

CountryColors <- c("orange", "red", "blue", "darkgreen")

text(pca$x[,1], pca$x[,2], colnames(x), col=CountryColors)
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
## or the second row here...
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

## Digging deeper (variable loadings)

```{r}
## Let's focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

# Q9: Generate a similar 'loadings plot' for PC2. What two food groups feature prominately and what does PC2 mainly tell us about?

Fresh potatoes is highest upward change, fresh fruit is highest downward change

## Biplots

```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```

